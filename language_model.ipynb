{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"06_language-models.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"F3UtqMMBbqhE"},"source":["La modélisation du langage consiste à apprendre la distribution de probabilité du mot suivant étant donné un historique. Ici, nous allons créer un modèle de langage sur des caractères pour apprendre à générer des titres de films de science fiction.\n","\n","Le jeu de données provient d'IMDB qui permet d'accèder à de nombreuses infos sur les films, et en plus donne ces données en téléchargement libre (http://www.imdb.com/interfaces/).\n","\n","Le fichier movies-sf.txt contient des noms de films suivis de leur année de sortie entre parenthèses extraits à partir de la base de données IMDB à l'aide de la commande awk en commentaire."]},{"cell_type":"code","metadata":{"id":"rzFKxRFJbqhI"},"source":["%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pEMubpsvbqhV","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1578928669745,"user_tz":-60,"elapsed":1358,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"ac193b69-2da3-4725-91b0-223ca6e67cec"},"source":["%%bash\n","#wget https://datasets.imdbws.com/title.basics.tsv.gz\n","#zcat title.basics.tsv.gz | awk -F\"\\t\" '$2==\"movie\" && $5==0 && /Sci-Fi/ && $6!=\"\\\\N\"{print $3\" (\"$6\")\"}' | iconv -f utf8 -t ascii//TRANSLIT | sort -u | shuf > movies-sf.txt\n","[ -f movies-sf.txt ] || wget -q https://raw.githubusercontent.com/benob/dl4nlp-tutorials-data/master/movies-sf.txt\n","head movies-sf.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Passengers (2016)\n","Stealth (2005)\n","Utterance (1997)\n","Homunculus, 6. Teil - Das Ende des Homunculus (1917)\n","Framework (2009)\n","Redshift (2013)\n","Jupiter 2023 (2018)\n","Fuerza maldita (1995)\n","Horrors of War (2006)\n","500 MPH Storm (2013)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"C16E1VmMbqhc"},"source":["Nous allons charger les titres caractère par caractère et encoder ces derniers sous forme d'entiers. Le vocabulaire est produit avec un `defaultdict` qui donne un nouvel identifiant à chaque nouveau caractère rencontré. Nous ajoutons deux caractères spéciaux : \n","- le symbole `<eos>` pour le padding\n","- le symbole `<start>` qui indique le début de la séquence\n","\n","Le problème va être posé comme prédire le caractère suivant étant donné le caractère courant et un état caché, et nous avons donc besoin d'un symbole `<start>` pour prédire le premier caractère. La fin d'un texte sera prédite par la première occurrence d'un symbole `<eos>`. \n","\n","Nous pouvons tout de suite créer un vocabulaire inversé pour vérifier le contenu des données chargées."]},{"cell_type":"code","metadata":{"id":"ou6EzeRrbqhe","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1578928669748,"user_tz":-60,"elapsed":1343,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"01bff5fc-7c04-4c05-aa49-789beefc5b84"},"source":["import collections\n","vocab = collections.defaultdict(lambda: len(vocab))\n","vocab['<eos>'] = 0\n","vocab['<start>'] = 1\n","\n","int_texts = []\n","with open('movies-sf.txt', 'r') as fp:\n","    for line in fp:\n","        int_texts.append([vocab['<start>']] + [vocab[char] for char in line.strip()])\n","\n","rev_vocab = {y: x for x, y in vocab.items()}\n","\n","print(rev_vocab)\n","print(len(int_texts))\n","\n","print(int_texts[42])\n","print(''.join([rev_vocab[x] for x in int_texts[42]]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{0: '<eos>', 1: '<start>', 2: 'P', 3: 'a', 4: 's', 5: 'e', 6: 'n', 7: 'g', 8: 'r', 9: ' ', 10: '(', 11: '2', 12: '0', 13: '1', 14: '6', 15: ')', 16: 'S', 17: 't', 18: 'l', 19: 'h', 20: '5', 21: 'U', 22: 'c', 23: '9', 24: '7', 25: 'H', 26: 'o', 27: 'm', 28: 'u', 29: ',', 30: '.', 31: 'T', 32: 'i', 33: '-', 34: 'D', 35: 'E', 36: 'd', 37: 'F', 38: 'w', 39: 'k', 40: 'R', 41: 'f', 42: '3', 43: 'J', 44: 'p', 45: '8', 46: 'z', 47: 'W', 48: 'M', 49: 'v', 50: 'A', 51: 'N', 52: '4', 53: 'B', 54: 'V', 55: 'I', 56: 'L', 57: 'G', 58: 'b', 59: 'C', 60: ':', 61: 'X', 62: 'x', 63: 'y', 64: 'O', 65: 'Z', 66: 'j', 67: 'q', 68: 'Y', 69: \"'\", 70: '?', 71: 'Q', 72: '/', 73: '&', 74: 'K', 75: '!', 76: '=', 77: '_', 78: '+', 79: ';', 80: '@', 81: '#', 82: '%', 83: '$'}\n","7205\n","[1, 31, 19, 5, 9, 2, 32, 6, 39, 9, 59, 19, 32, 67, 28, 32, 17, 3, 4, 9, 10, 13, 23, 45, 24, 15]\n","<start>The Pink Chiquitas (1987)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DKU2EC8aePvx","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1578928669751,"user_tz":-60,"elapsed":1329,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"0ee0cb57-7ffb-4f15-d70e-28393e4dc8d5"},"source":["a = 22\n","print(''.join([rev_vocab[x] for x in int_texts[a]]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<start>Viper (1996)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"n3IDF3rqbqhn"},"source":["Afin de bien choisir la longueur maximale sur laquelle le modèle va être entrainé, affichons l'histograme des longueurs de séquences."]},{"cell_type":"code","metadata":{"id":"GLTeAswBbqhp","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1578928669754,"user_tz":-60,"elapsed":1318,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"36d08ec7-ca86-4e03-fc7e-0246d9e4aa2a"},"source":["from matplotlib import pyplot as plt\n","\n","plt.hist([len(text) for text in int_texts])\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"A5U0MDodbqhu"},"source":["Il semble qu'une longeur maximale de 40 permettra de traiter une bonne partie des titres."]},{"cell_type":"code","metadata":{"id":"NZjp3vDQbqhx"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","\n","max_len = 40\n","batch_size = 8\n","embed_size = 16\n","hidden_size = 64"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I7b84aYTbqh4"},"source":["Le problème est similaire à un problème de tagging sauf que l'étiquette à prédire est le caractère suivant, donc nous devons agencer les tenseurs de manière à ce que $y_{t} = x_{t+1}$. Il faut calculer la longueur après coupure des séquences les plus longues, puis créer un tenseur à partir du texte pour $x$ et un tenseur à partir du texte décalé de 1 vers la gauche pour $y$.\n","\n","N'oublions pas de vérifier que les données ont la bonne forme."]},{"cell_type":"code","metadata":{"id":"PW7by_P4bqh5","colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1578928672834,"user_tz":-60,"elapsed":4388,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"1cad3557-16bd-4c43-8a65-69c8a742bf34"},"source":["X = torch.zeros(len(int_texts), max_len).long()\n","Y = torch.zeros(len(int_texts), max_len).long()\n","\n","for i, text in enumerate(int_texts):\n","    length = min(max_len, len(text) - 1) + 1\n","    X[i,:length - 1] = torch.LongTensor(text[:length - 1])\n","    Y[i,:length - 1] = torch.LongTensor(text[1:length])\n","\n","print(X[42].tolist())\n","print(Y[42].tolist())\n","print([rev_vocab[y] for y in Y[42].tolist()])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1, 31, 19, 5, 9, 2, 32, 6, 39, 9, 59, 19, 32, 67, 28, 32, 17, 3, 4, 9, 10, 13, 23, 45, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[31, 19, 5, 9, 2, 32, 6, 39, 9, 59, 19, 32, 67, 28, 32, 17, 3, 4, 9, 10, 13, 23, 45, 24, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","['T', 'h', 'e', ' ', 'P', 'i', 'n', 'k', ' ', 'C', 'h', 'i', 'q', 'u', 'i', 't', 'a', 's', ' ', '(', '1', '9', '8', '7', ')', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fKG8nTB8bqiA"},"source":["Nous découpons les données en un ensemble d'entraînement et un ensemble de validation, puis les outils pytorch pour créer des batches mélangés sont utilisés comme d'habitude."]},{"cell_type":"code","metadata":{"id":"cqVASBNKbqiF"},"source":["X_train = X[:6500]\n","Y_train = Y[:6500]\n","X_valid = X[6500:]\n","Y_valid = Y[6500:]\n","\n","from torch.utils.data import TensorDataset, DataLoader\n","train_set = TensorDataset(X_train, Y_train)\n","valid_set = TensorDataset(X_valid, Y_valid)\n","\n","train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","valid_loader = DataLoader(valid_set, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kFEklk3CbqiI"},"source":["Le modèle ressemble beaucoup à un taggeur. La première différence est qu'il ne peut pas être bidirectionnel, puisque la causalité est importante (on va générer des textes caractère par caratère en partant de `<start>`). La seconde différence est que la fonction `forward` va prendre un nouveau paramètre optionnel, l'état caché au temps précédent, et renvoyer non seulmenent les scores générés par le modèle, mais le nouvel état caché après avoir vu la séquence représentée dans `x`. Ceci sera nécessire pour la génération caractère par caractère. "]},{"cell_type":"code","metadata":{"id":"cJzaHYrmbqiK","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1578933371103,"user_tz":-60,"elapsed":1266,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"774300e6-cbdf-4e99-dc24-d9804f67806c"},"source":["class LM(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.embed = nn.Embedding(len(vocab), embed_size, padding_idx=vocab['<eos>'])\n","        self.rnn = nn.GRU(embed_size, hidden_size,  bias=False, num_layers=3, batch_first=True)\n","        self.dropout = nn.Dropout(0.3)\n","        self.decision = nn.Linear(hidden_size, len(vocab))\n","    \n","    def forward(self, x, h_0=None):\n","        embed = self.embed(x)\n","        output, h_n = self.rnn(embed, h_0)\n","        return self.decision(self.dropout(output)), h_n\n","\n","model = LM()\n","model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LM(\n","  (embed): Embedding(85, 16, padding_idx=0)\n","  (rnn): GRU(16, 64, num_layers=3, bias=False, batch_first=True)\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (decision): Linear(in_features=64, out_features=85, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":136}]},{"cell_type":"markdown","metadata":{"id":"rvi3bNsIbqiP"},"source":["On peut tester le modèle sur un batch. Il n'est pas obligatoire de passer un état caché initial (le module GRU s'en occupe si l'état caché passé est à `None`), mais on doit récupérer le nouvel état caché même si nous n'allons pas l'utiliser.\n","\n","Remarquons que les sorties sont de taille `(batch_size, sequence_length, num_labels)` et l'état caché `(num_layers, batch_size, hidden_size)`."]},{"cell_type":"code","metadata":{"id":"jzePxj9mbqiT","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1578933377840,"user_tz":-60,"elapsed":1324,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"44156f2b-e513-4c8e-a634-2f383a99bbe5"},"source":["output, hidden = model(Variable(X[:2]))\n","print(output.size(), hidden.size())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([2, 40, 85]) torch.Size([3, 2, 64])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6Y27sHCFbqiX"},"source":["Il n'y a très peu de différences avec l'évaluation des performances pour un taggeur. Il faut penser que le modèle renvoie maintenant deux résultats (les scores et l'état caché) et donc mettre l'état caché dans une variable qui ne sert à rien.\n","\n","À la place du taux de corrects, nous allons calculer la perplexité du modèle sur les données.\n","\n","$\n","PP(x) = P(x)^{-\\frac{1}{N}} = \\left[\\prod_i P(x_i)\\right]^{-\\frac{1}{N}}\n","$\n","\n","où $x$ est une séquence de mots, $P(x)=\\prod_i P(x_i)$ est la probabilité donnée par le modèle à cette séquence, et $N$ est sa longueur. On peut réécrire ce calcul en domaine log :\n","\n","$\n","PP(x) = exp\\left(-\\frac{1}{N}\\sum_i \\log P(x_i)\\right)\n","$\n","\n","Il se trouve que la fonction de loss renvoie $-\\frac{1}{N}\\log P(x_i)$, donc il suffit de calculer l'exponentielle du loss moyen pour obtenir la perplexité. Cette perplexité n'est pas masquée pour éliminer le padding, donc elle est influencée par ce dernier (on ne pourrait pas profiter de la fonction de loss si l'on souhaitait ignorer le padding)."]},{"cell_type":"code","metadata":{"id":"TtAUj2l5bqiZ","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1578933385315,"user_tz":-60,"elapsed":3888,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"660d85b4-79a7-45a9-e91c-1708221700e5"},"source":["import math\n","\n","def perf(model, loader):\n","    criterion = nn.CrossEntropyLoss()\n","    model.eval()\n","    total_loss = num = 0\n","    for x, y in loader:\n","      with torch.no_grad():\n","        y_scores, _ = model(x)\n","        loss = criterion(y_scores.view(y.size(0) * y.size(1), -1), y.view(y.size(0) * y.size(1)))\n","        total_loss += loss.item()\n","        num += len(y)\n","    return total_loss / num, math.exp(total_loss / num)\n","\n","perf(model, valid_loader)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.5644016529651399, 1.7583953372532446)"]},"metadata":{"tags":[]},"execution_count":138}]},{"cell_type":"markdown","metadata":{"id":"A_aSYe0tbqid"},"source":["L'apprentissage est le même que pour le taggeur sauf qu'il faut prendre en compte l'état caché."]},{"cell_type":"code","metadata":{"id":"jOEoVla5bqie","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1578933667645,"user_tz":-60,"elapsed":278772,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"8b0b5051-0206-4a9c-92ab-c9dbf3129a29"},"source":["def fit(model, epochs):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters())\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = num = 0\n","        for x, y in train_loader:\n","            optimizer.zero_grad()\n","            y_scores, _ = model(x)\n","            loss = criterion(y_scores.view(y.size(0) * y.size(1), -1), y.view(y.size(0) * y.size(1)))\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","            num += len(y)\n","        print(epoch, total_loss / num, *perf(model, valid_loader))\n","\n","fit(model, 10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 0.21763772565584916 0.16741039042777203 1.1822393454761435\n","1 0.16355197846889497 0.15341309139914547 1.1658064641105925\n","2 0.15487368035316468 0.14746466105711376 1.1588923300762426\n","3 0.15013941111931434 0.14351812465816524 1.1543277324130692\n","4 0.14618217221590188 0.13976594683126356 1.1500046051337411\n","5 0.14277459377508897 0.1374464711398943 1.1473402885104862\n","6 0.14021615049472222 0.13474061861105843 1.1442399513086423\n","7 0.13796364907117992 0.1328578309809908 1.1420876173122168\n","8 0.1359291507555888 0.13130233727448376 1.1403124881702318\n","9 0.13415535281254695 0.13016762302276935 1.1390192931806469\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"uDsqrq5kbqim"},"source":["Écrivons maintenant une fonction de génération. Cette dernère créé un tenseur $x$ contenant le symbole `<start>`, et un état caché à 0. Puis, elle repète l'application du modèle sur $x$ et l'état caché, pour générer un nouvel état caché et un vecteur de $y_{\\mathrm{scores}}$ sur les caractères. On peut alors sélectionner la composante de plus haut score, l'afficher et mettre à jour $x$ pour qu'il contienne le symbole généré. Il suffit ensuite de boucler jusqu'à la génération de `<eos>`.\n","\n","Le modèle génère toujours la même séquence de caractères, la séquence la plus probable étant donné le corpus d'apprentissage."]},{"cell_type":"code","metadata":{"id":"lu0mRdkkbqin","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1578933697432,"user_tz":-60,"elapsed":669,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"083bf6f9-f587-4b39-e83b-d9f735188bc6"},"source":["def generate_most_probable(model):\n","    x = torch.zeros((1, 1)).long()\n","    x[0, 0] = vocab['<start>']\n","    # size for hidden: (batch, num_layers * num_directions, hidden_size)\n","    hidden = torch.zeros(3, 1, hidden_size)\n","    with torch.no_grad():\n","      for i in range(200):\n","        y_scores, hidden = model(x, hidden)\n","        y_pred = torch.max(y_scores, 2)[1]\n","        selected = y_pred.data[0, 0].item()\n","        if selected == vocab['<eos>']:\n","            break\n","        print(rev_vocab[selected], end='')\n","        x[0, 0] = selected\n","    print()\n","\n","generate_most_probable(model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The Star (2018)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Nvl1N3Qdbqir"},"source":["Plutôt que de sélectionner le caractère ayant la plus grande probabilité, on peut tirer aléatoirement un caractère dans la distribution de probabilité après softmax. Utilisez `F.softmax` et `torch.multinomial` pour tirer aléatoirement un élément $s \\sim softmax(y_{\\textrm{scores}})$ dans la distribution des scores, et l'utiliser comme élément sélectionné à la place de celui issu du $max$.\n","\n","On peut diviser les scores par une température $\\theta$ avant de faire le softmax pour tasser la distriution. Une valeur de $\\theta<1$ poussera le modèle à prendre moins de risque et générer des caractères plus probables, alors que $\\theta>1$ lui fera prendre plus de risques et générer des séquences moins probables. En général, $\\theta=0.7$ donne des résultats satisfaisants.\n","\n","- Générez 100 séquences avec cette méthode.\n","- Modifier la fonction pour qu'elle prenne en entrée une chaîne de caractères représentant le début d'un titre (par exemple \"Star \"), et force le réseau à passer par ces caractères (sans oublier `<start>`) avant de générer une fin pour le titre.\n","\n"]},{"cell_type":"code","metadata":{"id":"f4Knx-Llbqis","tags":["solution"],"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1578934361927,"user_tz":-60,"elapsed":1694,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"cc3c510e-186a-4342-ef11-87553b6b8acc"},"source":["def generate_word(model, start, theta):\n","    x = torch.zeros((1, 1)).long()\n","    x[0, 0] = vocab['<start>']\n","    # size for hidden: (batch, num_layers * num_directions, hidden_size)\n","    hidden = torch.zeros(3, 1, hidden_size)\n","    with torch.no_grad():\n","      for ch in start:\n","        y_scores, hidden = model(x, hidden)\n","        x[0, 0] = vocab.get(ch)\n","        print(ch, end='')\n","      for i in range(200):\n","        y_scores, hidden = model(x, hidden)\n","        test = F.softmax(y_scores/theta,-1)\n","        y_pred = torch.multinomial(test[0], 2)\n","        selected = y_pred.data[0, 0].item()\n","        if selected == vocab['<eos>']:\n","            break\n","        print(rev_vocab[selected], end='')\n","        x[0, 0] = selected\n","    print()\n","th = 0.5\n","for i in range(100):    \n","  generate_word(model,'Star ', th)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Star Dart (2018)\n","Star (2017)\n","Star Come (2008)\n","Star Dark (2018)\n","Star Space (2017)\n","Star (2018)\n","Star Horner (2016)\n","Star Tors (2017)\n","Star Tark (1988)\n","Star Twer (2017)\n","Star Spot (2007)\n","Star Man (2015)\n","Star Fatien (1986)\n","Star Seland (2007)\n","Star (2016)\n","Star Shark (2018)\n","Star II (2012)\n","Star Brojon (2017)\n","Star Master (1989)\n","Star Malend (2017)\n","Star Warra (2015)\n","Star Anchine (1997)\n","Star Word (2014)\n","Star Hor (2018)\n","Star Silan (2016)\n","Star Wimor (1998)\n","Star Man (2018)\n","Star Ariend (2018)\n","Star World (2016)\n","Star Tand (2018)\n","Star (2012)\n","Star Srider (2006)\n","Star Grand (2018)\n","Star (2018)\n","Star Serop (1998)\n","Star World (2015)\n","Star One (1998)\n","Star Senger (2018)\n","Star One (2011)\n","Star Montrigres (2018)\n","Star Men (2012)\n","Star One (2006)\n","Star (1961)\n","Star Man (1959)\n","Star Saparion (2018)\n","Star (2017)\n","Star Brosite (2016)\n","Star II (2018)\n","Star (2018)\n","Star Kark (2011)\n","Star (1996)\n","Star Purm (2018)\n","Star (2009)\n","Star World (2015)\n","Star Skerender (2016)\n","Star World (2005)\n","Star (2017)\n","Star (2018)\n","Star Reborg (2015)\n","Star Dead (2007)\n","Star World (2014)\n","Star Soms (2012)\n","Star War (1986)\n","Star (2004)\n","Star War (2018)\n","Star Forst (2011)\n","Star Antrant (1986)\n","Star (2018)\n","Star Shart (2018)\n","Star Alien Cary (2017)\n","Star Earth (2016)\n","Star Time (2018)\n","Star Brom Arieng (1998)\n","Star Honster Milra (2010)\n","Star World (2018)\n","Star Comper (2017)\n","Star (2013)\n","Star Dester (2018)\n","Star (2008)\n","Star (2018)\n","Star Sap a (2017)\n","Star Invanl (2016)\n","Star Pare (2018)\n","Star Tark (2014)\n","Star Frondis (2017)\n","Star Time (1987)\n","Star Srics (2018)\n","Star Mars (2018)\n","Star Barthen (2008)\n","Star Tarler (2018)\n","Star Antant (2008)\n","Star Rand (2015)\n","Star Sermer (1998)\n","Star World (1989)\n","Star (2016)\n","Star Moarth (1995)\n","Star The Antrelel (2016)\n","Star Lam (2015)\n","Star (2017)\n","Star (2018)\n"],"name":"stdout"}]}]}