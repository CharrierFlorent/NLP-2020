{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copie de 07_encoder-decoder-attention.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jqILd8iTb1QM"},"source":["Nous nous intéressons maintenant à un problème de transduction d'une séquence de symbole en une autre (autrement dit de la traduction). Traduire de séquences de mots d'une langue à une autre n'est pas raisonnable sans GPU. Nous allons donc nous intéresser au problème de la phonétisation automatique. Étant donné une séquence de graphèmes, nous devons générer la séquence de phonèmes correspondant.\n","\n","Ce problème peut être vu comme la composition de deux problèmes vus dans les notebooks précédents :\n","- le problème de prédiction de la polarité d'un tweet : lire une séquence en entrée et produire une représentation à partir de cette séquence (ici les graphèmes)\n","- le problème de modèlisation du langage : partir d'une représentation cachée puis générer une séquence de symboles (ici les phonèmes)\n","\n","On appelle souvent ce cadre \"encodeur-décodeur\" ou \"seq2seq2.\n","\n","Comme les modèles de langages conditionnés (par leur état caché initial) ne fonctionnent pas très bien car ils doivent emagasiner toute l'information sur la séquence vue en entrée dans une représentation de taille fixe, nous dans une deuxième étape augmenter le modèle d'un macanisme d'attention.\n","\n","Commençons par télécharger un dictionnaire phonétisé de petite taille créé à partir d'un sous-ensemble du dictionnaire de CMU (utilisé dans l'ASR sphinx). Ce dictionnaire, regénérable avec les commandes en commentaire, contient sur chaque ligne un mot, suivi d'un séparateur \"|||\" suivi d'une sèquence de phonèmes."]},{"cell_type":"code","metadata":{"id":"SuL1XnCt2CqC","executionInfo":{"status":"ok","timestamp":1613091770391,"user_tz":-60,"elapsed":1473,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}}},"source":["%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"uJ6iMgAGb1Qf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613091771153,"user_tz":-60,"elapsed":2224,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"0bf0a0ea-21b1-4670-b096-8e8e5426df2e"},"source":["%%bash\n","# wget -q http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b\n","# iconv -f latin1 cmudict-0.7b | grep \"^[A-Z]\" | awk 'NF < 16 {print}' | sed 's/([0-9]*)//;s/  / ||| /' | shuf | head -5500 > cmudict-0.7b.filtered\n","[ -f cmudict-0.7b.filtered ] || wget -q https://raw.githubusercontent.com/benob/dl4nlp-tutorials-data/master/cmudict-0.7b.filtered\n","head cmudict-0.7b.filtered"],"execution_count":2,"outputs":[{"output_type":"stream","text":["VICTIMIZING ||| V IH1 K T AH0 M AY0 Z IH0 NG\n","BURG ||| B ER1 G\n","CORDREY ||| K AO1 R D R IY0\n","ESPY'S ||| EH1 S P IY0 Z\n","REPUDIATES ||| R IY0 P Y UW1 D IY0 EY2 T S\n","DUN ||| D AH1 N\n","LEVELING ||| L EH1 V AH0 L IH0 NG\n","REFLATE ||| R IY0 F L EY1 T\n","MOSSAD ||| M OW0 S AA1 D\n","MOVIEGOER ||| M UW1 V IY2 G OW2 ER0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dEnfOy2zb1Qp"},"source":["Le chargement des données nécessite de convertir les mots en listes de caractères, et la séquence de phonème en liste de chaînes de caractères."]},{"cell_type":"code","metadata":{"id":"bB5D__eC2fnT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613091771154,"user_tz":-60,"elapsed":2219,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"6d77d1bd-573d-4554-d516-c58a4d0b1f53"},"source":["words = []\n","phonemes = []\n","\n","with open(\"cmudict-0.7b.filtered\") as fp:\n","    for line in fp:\n","        word, phones = line.strip().split(' ||| ')\n","        words.append(list(word))\n","        phonemes.append(phones.split())\n","\n","print(words[42], phonemes[42])\n","len(words)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["['D', 'I', 'S', 'T', 'R', 'E', 'S', 'S'] ['D', 'IH0', 'S', 'T', 'R', 'EH1', 'S']\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["5500"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"Q6Utwc_ub1Qy"},"source":["Si on regarde la distribution des tailles de mots et de phonétisations, on peut voir que l'on couvre la plupart des cas avec une longueur maximale de 16."]},{"cell_type":"code","metadata":{"id":"Z7hzn2t4b1Q0","colab":{"base_uri":"https://localhost:8080/","height":513},"executionInfo":{"status":"ok","timestamp":1613091771156,"user_tz":-60,"elapsed":2214,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"3455c97f-52d9-470b-c08d-356dcca8ddd7"},"source":["from matplotlib import pyplot as plt\n","\n","plt.hist([len(x) for x in words])\n","plt.show()\n","plt.hist([len(x) for x in phonemes])\n","plt.show()"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"erIr_oJyb1Q7"},"source":["La conversion des entrées et sorties du système en séquences d'entiers se fait comme pour l'analyse de sentiment et le modèle de langage. Notez la présence du symbole `<start>` pour la partie modèle de langage."]},{"cell_type":"code","metadata":{"id":"yKPsumQH_dsz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613091771473,"user_tz":-60,"elapsed":2526,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"eaa6ef80-bcd3-4f62-863d-6b956fea786a"},"source":["import collections\n","\n","letter_vocab = collections.defaultdict(lambda: len(letter_vocab))\n","letter_vocab['<eos>'] = 0\n","\n","phoneme_vocab = collections.defaultdict(lambda: len(phoneme_vocab))\n","phoneme_vocab['<eos>'] = 0\n","phoneme_vocab['<start>'] = 1\n","\n","int_words = []\n","int_phonemes = []\n","\n","for word, phones in zip(words, phonemes):\n","    int_words.append([letter_vocab[x] for x in word])\n","    int_phonemes.append([phoneme_vocab[x] for x in phones])\n","\n","print(len(letter_vocab), len(phoneme_vocab))\n","print(int_words[42], int_phonemes[42])\n","\n","rev_letter_vocab = {y: x for x, y in letter_vocab.items()}\n","rev_phoneme_vocab = {y: x for x, y in phoneme_vocab.items()}\n","\n","print([rev_letter_vocab[x] for x in int_words[42]], [rev_phoneme_vocab[x] for x in int_phonemes[42]])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["35 71\n","[13, 2, 16, 4, 11, 14, 16, 16] [17, 10, 20, 5, 16, 19, 20]\n","['D', 'I', 'S', 'T', 'R', 'E', 'S', 'S'] ['D', 'IH0', 'S', 'T', 'R', 'EH1', 'S']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XpiTofrqb1RC"},"source":["Nous allons utiliser des hyperparamètres de magnitude réduite pour pouvoir entraîner le système sur CPU. Sur GPU, on pourrait prendre de bien plus grands états cachés. De plus, rien ne nous empêche d'avoir des tailles d'embedding et d'état caché différentes selon que l'on est dans la partie \"encodeur\" ou \"décodeur\"."]},{"cell_type":"code","metadata":{"id":"CZ9NUarqnG2M","executionInfo":{"status":"ok","timestamp":1613091774668,"user_tz":-60,"elapsed":5719,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","max_len = 16\n","batch_size = 16\n","embed_size = 16\n","hidden_size = 32"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"obbRiDUcb1RM"},"source":["Une fois que l'on a des listes de listes d'entiers, il est relativement simple de les mettre dans des tenseurs avec le padding habituel. Le chois de coller les séquences à gauche est complètement arbitraire."]},{"cell_type":"code","metadata":{"id":"p1SBrKZG3Qgq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613091774670,"user_tz":-60,"elapsed":5716,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"457fe62c-acdd-418d-db1a-ab1b660f2ef7"},"source":["X = torch.zeros((len(int_words), max_len)).long()\n","Y = torch.zeros((len(int_phonemes), max_len)).long()\n","\n","for i, (word, phones) in enumerate(zip(int_words, int_phonemes)):\n","    word_length = min(max_len, len(word))\n","    X[i,0:word_length] = torch.LongTensor(word[:word_length])\n","    phones_length = min(max_len, len(phones))\n","    Y[i,0:phones_length] = torch.LongTensor(phones[:phones_length])\n","\n","print(X[42].tolist())\n","print(Y[42].tolist())"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[13, 2, 16, 4, 11, 14, 16, 16, 0, 0, 0, 0, 0, 0, 0, 0]\n","[17, 10, 20, 5, 16, 19, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NAT1YPm0b1RS"},"source":["Le corpus est divisé en un ensemble d'entraînement et de validation, et nous utilisons les facilités proposées par pytorch pour la génération des batches."]},{"cell_type":"code","metadata":{"id":"A7wKQ6MH4pXE","executionInfo":{"status":"ok","timestamp":1613091774670,"user_tz":-60,"elapsed":5714,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}}},"source":["X_train = X[:5000]\n","Y_train = Y[:5000]\n","X_valid = X[5000:]\n","Y_valid = Y[5000:]\n","\n","from torch.utils.data import TensorDataset, DataLoader\n","train_set = TensorDataset(X_train, Y_train)\n","valid_set = TensorDataset(X_valid, Y_valid)\n","\n","train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","valid_loader = DataLoader(valid_set, batch_size=batch_size)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MFI6jV8sb1RY"},"source":["Voici notre premier modèle. C'est un encodeur-decodeur classique qui utilise une couche d'embedding pour projeter les caractères du mot vers un espace d'embedding, puis une couche recurrente bidirectionnelle pour créer une représentation de l'intégralité du mot. La représentation issue de cette couche sera de taille `(num_layers * num_directions, batch_size, hidden_size)`, donc il faut que la seconde couche recurrente qui va générer les phonèmes ait une couche cachée de taille `2 * hidden_size`. Cette dernière est construite comme un modèle de langage : elle commence par le symbole `<start>`, le projette dans un espace d'embedding, le passe dans la couche recurrent, puis la sortie de cette dernière dans une couche de décision qui génère un vecteur de scores de la taille du vocabulaire des phonèmes.\n","\n","L'inférence est divisée en deux, la partie encodage qui renvoie l'état caché à l'issue de la lecture du mot, et la partie décodage qui renvoie la décision à chaque position pour la séquence de phonème ainsi que l'état caché à la fin (pour pouvoir faire un décodage phonème par phonème comme on l'a fait dans le modèle de langage)."]},{"cell_type":"code","metadata":{"id":"K0KV7c3Pb1Rc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613091775205,"user_tz":-60,"elapsed":6243,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"a5abdb3f-d910-42c1-e862-f62ae8f25c97"},"source":["class Seq2SeqModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.letter_embed = nn.Embedding(len(letter_vocab), embed_size, padding_idx=letter_vocab['<eos>'])\n","        self.phoneme_embed = nn.Embedding(len(phoneme_vocab), embed_size)\n","        self.letter_rnn = nn.GRU(embed_size, hidden_size, num_layers=1, bias=False, bidirectional=True, batch_first=True)\n","        self.phoneme_rnn = nn.GRU(embed_size, 2 * hidden_size, num_layers=1, batch_first=True)\n","        self.dropout = nn.Dropout(0.3)\n","        # size of hidden state: (num_layers * num_directions, batch_size, hidden_size)\n","        self.decision = nn.Linear(hidden_size * 2 * 1, len(phoneme_vocab))\n","    \n","    def encode(self, word):\n","        embed = self.letter_embed(word)\n","        output, h_n = self.letter_rnn(embed)\n","        return self.dropout(h_n.transpose(0, 1).contiguous().view(1, word.size(0), -1))\n","    \n","    def decode(self, phones, h_0):\n","        embed = self.phoneme_embed(phones)\n","        output, h_n = self.phoneme_rnn(embed, h_0)\n","        return self.dropout(self.decision(output)), h_n\n","    \n","    def forward(self, word, phones):\n","        output, h_n = self.decode(phones, self.encode(word))\n","        return output\n","\n","seq2seq_model = Seq2SeqModel()\n","seq2seq_model"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2SeqModel(\n","  (letter_embed): Embedding(35, 16, padding_idx=0)\n","  (phoneme_embed): Embedding(71, 16)\n","  (letter_rnn): GRU(16, 32, bias=False, batch_first=True, bidirectional=True)\n","  (phoneme_rnn): GRU(16, 64, batch_first=True)\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (decision): Linear(in_features=64, out_features=71, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"JTWVbtB6b1Rg"},"source":["On peut vérifier que le modèle renvoie bien un tenseur de taille `(batch_size, sequence_length, num_phonemes)`. Pour celà nous passons $Y$ mais ce dernier représente les phonèmes à générer, pas les phonèmes précédents."]},{"cell_type":"code","metadata":{"id":"u1pG-mzlb1Ri","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613091775206,"user_tz":-60,"elapsed":6241,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"16726994-080a-4d23-b8e1-d11c4d45f689"},"source":["with torch.no_grad():\n","  print(seq2seq_model(X[:3], Y[:3]).size())"],"execution_count":10,"outputs":[{"output_type":"stream","text":["torch.Size([3, 16, 71])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xKRmFtEyb1Rm"},"source":["L'évaluation sur les données de validation peut renvoyer la perplexité (ou un taux d'erreur). Par contre, le `loader` renvoie des paires $(x, y)$ contenant des batches de mots et phonétisations correspondantes. Donc il est nécessaire de créer une nouvelle variable qui contient les phonèmes décalés vers la gauche (phonème précédent) précédés du symbole `<start>`. Pour conserver la taille de séquence, on utilise le sous-tenseur `y[:,:-1]` qui représente tous les éléments de y sauf le dernier (sur la dimension 1), pour le batch en intégralité (dimension 0). "]},{"cell_type":"code","metadata":{"id":"66quaE7vb1Rn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613091775209,"user_tz":-60,"elapsed":6238,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"77ab7bc3-cb4c-49d4-817d-64678fa5e90c"},"source":["import math\n","\n","def perf(model, loader):\n","    criterion = nn.CrossEntropyLoss()\n","    model.eval()\n","    total_loss = num = 0\n","    for x, y in loader:\n","      with torch.no_grad():\n","        x2 = torch.cat([phoneme_vocab['<start>'] * torch.ones(y.size(0), 1).long(), y[:,:-1]], 1)\n","        y_scores = model(x, x2)\n","        loss = criterion(y_scores.view(y.size(0) * y.size(1), -1), y.view(y.size(0) * y.size(1)))\n","        total_loss += loss.item()\n","        num += len(y)\n","    return total_loss / num, math.exp(total_loss / num)\n","\n","perf(seq2seq_model, valid_loader)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.2801106538772583, 1.3232762298819731)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"NyNqQW-pb1Rs"},"source":["Il est alors nécessaire de modifier la fonction d'apprentissage du modèle de langage de la même manière."]},{"cell_type":"code","metadata":{"id":"8U4spqTDb1Rt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613091815723,"user_tz":-60,"elapsed":46748,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"8ce5479e-e998-41e6-aca3-74203d2cb93a"},"source":["def fit(model, epochs):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters())\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = num = 0\n","        for x, y in train_loader:\n","            x2 = torch.cat([phoneme_vocab['<start>'] * torch.ones(y.size(0), 1).long(), y[:,:-1]], 1)\n","            optimizer.zero_grad()\n","            y_scores = model(x, x2)\n","            loss = criterion(y_scores.view(y.size(0) * y.size(1), -1), y.view(y.size(0) * y.size(1)))\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","            num += len(y)\n","        print(epoch, total_loss / num, *perf(model, valid_loader))\n","\n","fit(seq2seq_model, 10)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["0 0.1443101969242096 0.0947304117679596 1.0993624399963124\n","1 0.12513913564682008 0.08375990653038025 1.0873677925596597\n","2 0.11777140786647797 0.07628635668754577 1.0792715865477762\n","3 0.11372308540344238 0.07113447678089142 1.0737256073233967\n","4 0.11003076860904694 0.06732555997371674 1.069643654655458\n","5 0.10778649117946625 0.06443848776817322 1.0665599697426908\n","6 0.10538489055633544 0.061552523851394654 1.0634863534610657\n","7 0.10442356967926025 0.05869270086288452 1.0604493155048427\n","8 0.10185888698101044 0.05682327938079834 1.058468740615196\n","9 0.10149811503887177 0.05502121818065643 1.0565630327829623\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kAOZW9gub1Rw"},"source":["Nous pouvons faire une fonction de génération pour ce modèle. La différence avec le modèle de langage est que nous commençon avec un état caché généré par l'encodeur, et que la fonction `decode()` permet de générer la séquence phonème par phonème.\n","\n","Une fois le modèle entraîné, on s'apperçoit que le génèrateur n'est pas si bon. En général, il commence bien les phonétisations mais n'arrive pas à les terminer. Il ajoute souvent des sons qui n'apparaissent pas. Ce phénomène est dû à deux problèmes :\n","- l'état caché qui sert à encoder le mot entier est limité et partagé avec celui utilisé pour le décodage (on pourrait concaténer les embeddings des phonèmes passés en entrée avec une copie de cet état caché à chaque étape, pour ne pas perdre la mémoire). Entraîner le modèle plus longtemps avec plus de données et un plus grand état caché pourrait améliorer la situtation.\n","- il y a une différence entre les conditions d'apprentissage et de prédiction car en apprentissage on utilise le symbole précédent de référence (méthode \"teacher forcing\") alors qu'en test, on utilise le symbole prédit, potentiellement faux. Des méthodes ont été proposées pour passer de la distribution forcée à la distribution réelle au court de l'apprentissage, mais c'est compliqué à mettre en oeuvre.\n","\n","Heureusement, il y a les mécanismes d'attention."]},{"cell_type":"code","metadata":{"id":"pVxIc5nbb1Ry","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613091816469,"user_tz":-60,"elapsed":47490,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"2c997442-6cab-430a-b62b-7726ac04784c"},"source":["def generate_seq2seq(model, word):\n","    int_word = [letter_vocab[letter] for letter in word]\n","    x = torch.LongTensor(int_word).view(1, -1)\n","    hidden = model.encode(x)\n","    \n","    x2 = torch.zeros((1, 1)).long()\n","    x2[0, 0] = phoneme_vocab['<start>']\n","\n","    with torch.no_grad():\n","      for i in range(200):\n","        y_scores, hidden = model.decode(x2, hidden)\n","        y_pred = torch.max(y_scores, 2)[1]\n","        selected = y_pred.data[0, 0].item()\n","        if selected == phoneme_vocab['<eos>']:\n","            break\n","        print(rev_phoneme_vocab[selected], end=' ')\n","        x2[0, 0] = selected\n","    print()\n","\n","generate_seq2seq(seq2seq_model, 'BONJOUR')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["B R AH1 N T AH0 N D IH0 K \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OELrxPaub1R2"},"source":["Les mechanismes d'attention reprennent l'idée provenant de la cognition humaine que nous pouvons porter notre attention sur un sous-ensemble des entrées plutôt que devoir en appréhender l'integralité en permanence. Pour un problème `seq2seq`, ceci va se traduire en l'utilisation sélective des états cachés des caractères du mot en entrée en fonction du phonème que l'on est en train de générer, plutôt que de prendre le dernier état caché de la séquence.\n","\n","L'encodeur va cette fois renvoyer les sorties du RNN (son état caché à chaque indice) plutôt que le dernier état caché. Le décodeur les sorties du RNN sur les phonèmes, puis transforme ces sorties avec une couche linéaire appelée `attn`. On réalise la multiplication de matrice entre cette sortie transformée et chacun des état cachés sur l'entrée (les caractères du mot) et l'on prend le softmax du résultat (ce type d'attention est appelé attention multiplicative). Ceci donne une distribution sur les entrée que l'on appelle poids d'attention. On peut alors calculer la somme pondérée des états cachés en entrée par ces poids d'attention pour obtenir une représentation de l'entrée contextuelle pour le décodage du phonème courant. C'est la concaténation de cette représentation contextuelle et de la sortie du RNN sur les phonèmes qui est utilisée pour prendre la décision finale.\n","\n","Les choses se compliquent un peu car nous faisons des traitements par batch, et donc les séquences de caractères contiennent du padding. Même si l'on applique la technique vue précédemment pour que l'état caché correspondant aux symboles de padding soit nul, le mechanisme d'attention risque d'utiliser les sorties du RNN à cet endroit pour apprendre des régularités sur la distribution a priori ou la longueur des entrées. Il faut donc s'assurer que le softmax donnera un poids de zéro aux états cachés issus du padding. On utilise un masque calculé sur les entrées par l'encodeur qui est vrai pour chaque symbole de padding, faux sinon. La fonction `masked_fill_()` permet alors de fixer les poids d'attention à $-\\infty$ avant de faire le softmax. Comme le numérateur de ce dernier prend l'exponencielle de ses entrées, on a bien un poids à zéro. Ceci permet aussi de couper la propagation du gradient pour ces composantes. \n","\n","Pour ce qui est de la taille des différentes couches, le RNN sur les caractères est bidirectionnel donc sa sortie est 2 fois la taille de la couche cachée. La couche de transformation s'occupe de projeter l'état caché du RNN sur les phonèmes qui est unidirectionnel (donc elle passe de `hidden_size` à `2 * hidden_size`). La multiplication de matrice traite des matrices de la taille `(batch_size, sequence_length, 2 * hidden_size)` et `(batch_size, 2 * hidden_size, sequence_length)` après transposition. Il en resulte donc une matrice de poids de taille `(batch_size, sequence_length, sequence_length)`. Comme nous avons la même taille de séquences pour les mots et les phonétisations, ce n'est pas facile à interpréter, mais celà correspond en fait à `(batch_size, phoneme_size, word_size)`, donc c'est bien sur la dimensions correspondant aux mots que l'on veut faire le softmax. Finalement, le contexte créé est la somme pondérée des états cachés sur les entrées, de taille `2 * hidden_size`, et l'état caché du RNN sur les phonèmes est de taille `hidden_size`, donc la couche de décision a une entrée de `3 * hidden_size`.\n","\n","Pour rappel sur les RNN :\n","- taille de l'état caché : `(num_layers * num_directions, batch_size, hidden_size)`\n","- taille de la sortie : `(batch_size, sequence_length, num_directions * hidden_size)`\n"]},{"cell_type":"code","metadata":{"id":"w4Ut9c0ab1R3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613091816470,"user_tz":-60,"elapsed":47487,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"931734b8-e2e2-40a3-b96c-7944e6d8b7db"},"source":["class AttnModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.letter_embed = nn.Embedding(len(letter_vocab), embed_size, padding_idx=letter_vocab['<eos>'])\n","        self.phoneme_embed = nn.Embedding(len(phoneme_vocab), embed_size)\n","        self.letter_rnn = nn.GRU(embed_size, hidden_size, num_layers=1, bias=False, bidirectional=True, batch_first=True)\n","        self.phoneme_rnn = nn.GRU(embed_size, hidden_size, num_layers=1, batch_first=True)\n","        self.dropout = nn.Dropout(0.3)\n","        self.attn = nn.Linear(hidden_size, hidden_size * 2)\n","        self.decision = nn.Linear(hidden_size * 3, len(phoneme_vocab))\n","    \n","    def encode(self, word):\n","        mask = (word == 0)\n","        embed = self.letter_embed(word)\n","        output, h_n = self.letter_rnn(embed)\n","        return self.dropout(output), mask\n","    \n","    def decode(self, phones, states, mask, h_0=None):\n","        embed = self.phoneme_embed(phones)\n","        output, h_n = self.phoneme_rnn(embed, h_0)\n","        output = self.dropout(output)\n","        \n","        a1 = self.attn(output)\n","        a2 = a1.bmm(states.transpose(1, 2))\n","        a2.data.masked_fill_(mask.unsqueeze(1).data, -float('inf'))\n","        attn_weights = F.softmax(a2, 2)\n","\n","        context = attn_weights.bmm(states)\n","        scores = self.decision(torch.cat([context, output], 2))\n","        return scores, h_n, attn_weights\n","    \n","    def forward(self, word, phones):\n","        states, mask = self.encode(word)\n","        output, h_n, attn_weights = self.decode(phones, states, mask)\n","        return output\n","\n","attn_model = AttnModel()\n","print(attn_model)\n","with torch.no_grad():\n","  print(attn_model(X[:3], Y[:3]).size())"],"execution_count":14,"outputs":[{"output_type":"stream","text":["AttnModel(\n","  (letter_embed): Embedding(35, 16, padding_idx=0)\n","  (phoneme_embed): Embedding(71, 16)\n","  (letter_rnn): GRU(16, 32, bias=False, batch_first=True, bidirectional=True)\n","  (phoneme_rnn): GRU(16, 32, batch_first=True)\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (attn): Linear(in_features=32, out_features=64, bias=True)\n","  (decision): Linear(in_features=96, out_features=71, bias=True)\n",")\n","torch.Size([3, 16, 71])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DZVh0_W3b1R8"},"source":["On peut entraîner ce modèle et l'on doit normalement obtenir des loss plus faibles sur l'ensemble de validation que pour l'encodeur-décodeur sans attention."]},{"cell_type":"code","metadata":{"id":"dTL4z3mhb1R9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613091858165,"user_tz":-60,"elapsed":89177,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"0b8d7b00-8081-4da7-bda3-0690ff22de91"},"source":["fit(attn_model, 10)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["0 0.11643417174816131 0.08848588824272156 1.0925188351410722\n","1 0.07587350167036057 0.06554660701751709 1.0677425004472554\n","2 0.06077448695898056 0.0510687096118927 1.052395200464858\n","3 0.05213540579080582 0.043433632969856265 1.044390678904254\n","4 0.04658712577819824 0.03801210045814514 1.0387438020783517\n","5 0.0422452921807766 0.03412593477964401 1.0347149051169666\n","6 0.03921630650162697 0.03133192354440689 1.031827935038314\n","7 0.03656509975790977 0.029417384326457977 1.0298543498464743\n","8 0.03459066832661629 0.027671908140182496 1.0280583315130927\n","9 0.03333232174515724 0.026547044336795805 1.0269025560750498\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_FlAsT-6b1SA"},"source":["\n","\n","On peut faire une fonction de génération sur le modèle de `generate_seq2seq()` pour le modèle avec attention. Il n'y a qu'à changer l'appel aux fonction `encode` et `decode` pour passer les bons paramètres, et produire un état caché de départ à zéro pour le décodeur comme dans le modèle de langage.\n","\n","Plutôt que de prendre le phonème le plus probable en décodage, nous pourrions le tirer aléatoirement dans la distribution de scores. Modifier la fonction en ce sens, et collectez des statistiques sur les résultats de 100 tirages pour le mot \"BONJOUR\". Quelle est la phonéistation la plus courrament générée ?"]},{"cell_type":"code","metadata":{"id":"S5UB6jnwb1SC","tags":["solution"],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613091952541,"user_tz":-60,"elapsed":452,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"e7149f39-6e3c-4285-cf33-879d222f0b48"},"source":["import numpy as np\n","import random\n","def generate_attn(model, word):\n","    int_word = [letter_vocab[letter] for letter in word]\n","    x = torch.LongTensor(int_word).view(1, -1)\n","    hidden, mask = model.encode(x)\n","    x2 = torch.zeros((1, 1)).long()\n","    x2[0, 0] = phoneme_vocab['<start>']\n","    h_n = None\n","    with torch.no_grad():\n","      for i in range(200):\n","        y_scores, h_n, weigths = model.decode(x2, hidden, mask, h_n)\n","        y_pred = torch.max(y_scores, 2)[1]\n","        selected = y_pred.data[0, 0].item()\n","        if selected == phoneme_vocab['<eos>']:\n","            break\n","        print(rev_phoneme_vocab[selected], end=' ')\n","        x2[0, 0] = selected\n","    print()\n","\n","generate_attn(attn_model, 'BONJOUR')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["B AA1 N B ER0 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"reFyKsxeb1SF"},"source":["On peut aussi demander au modèle de nous renvoyer la matrice d'attention pour pouvoir analyser les états cachés utilisés par le modèle pour faire ses prédictions. Il est intéressant de voir que le modèle apprend à ignorer les muettes. L'attention multiplicatie calcule une similarité entre les états cachés en entrée et en sortie (transformés) et a donc tendance à être forte lorsque les symboles sont systématiquement la traduction l'un de l'autre car le modèle peut apprendre une représentation similaire pour un caractère et un phonème."]},{"cell_type":"code","metadata":{"id":"WIXNwFvOb1SG","colab":{"base_uri":"https://localhost:8080/","height":275},"executionInfo":{"status":"ok","timestamp":1613091858167,"user_tz":-60,"elapsed":89174,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"804333b5-ad71-4243-843b-cc674748263c"},"source":["def show_attn(attn_model, word):\n","    int_word = [letter_vocab[letter] for letter in word]\n","    x = torch.LongTensor(int_word).view(1, -1)\n","    states, mask = attn_model.encode(x)\n","    \n","    x2 = torch.zeros((1, 1)).long()\n","    x2[0, 0] = phoneme_vocab['<start>']\n","    hidden = torch.zeros(1, 1, hidden_size)\n","    result = []\n","    attn_matrix = []\n","    with torch.no_grad():\n","      for i in range(200):\n","        y_scores, hidden, attn = attn_model.decode(x2, states, mask, hidden)\n","        attn_matrix.append(attn.squeeze().data.tolist())\n","        y_pred = torch.max(y_scores, 2)[1]\n","        selected = y_pred.data[0, 0].item()\n","        result.append(rev_phoneme_vocab[selected])\n","        if selected == phoneme_vocab['<eos>']:\n","            break\n","        x2[0, 0] = selected\n","    plt.matshow(attn_matrix)\n","    plt.xticks(range(len(word)), word)\n","    plt.yticks(range(len(result)), result)\n","    plt.show()\n","\n","show_attn(attn_model, 'THOROUGH')"],"execution_count":17,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 384x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}