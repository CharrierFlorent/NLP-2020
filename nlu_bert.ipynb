{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"08_nlu-bert.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"66dzAZ6ZgEK6"},"source":["On s'intéresse ici à la prédiction de l'intention (`intent`) d'un utilisateur dans un système de dialogue. Pour chaque ennoncé, il faut prédire la catégorie de l'ennoncé parmis les 7 intentions spécifiquse au domaine du corpus SNIPS : ajouter un élément à une playlist, réserver un réstaurent, obtenir la météo, écouter de la musique, donner son avis sur un livre, rechercher une oeuvre artistique ou obtenir des horaires de cinéma.\n","\n","Le corpus est divisé en 3 parties : train, valid et test. Il contient un fichier `seq.in` avec un ennoncé par ligne, `seq.out` avec des étiquettes BIO pour représenter les entités liées aux intentions (non-utilisé ici) et un fichier `label` contenant les intentions."]},{"cell_type":"code","metadata":{"id":"UZOp5OO5slzv","colab":{"base_uri":"https://localhost:8080/","height":382},"executionInfo":{"status":"ok","timestamp":1579789483310,"user_tz":-60,"elapsed":1292,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"eb3a0442-0a8e-43d7-ec67-0c448fdf9ce8"},"source":["%%bash\n","\n","curl -sO https://pageperso.lis-lab.fr/benoit.favre/files/snips-nlu.zip\n","unzip -o snips-nlu.zip\n","sort snips-nlu/train/label | uniq -c"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  snips-nlu.zip\n","   creating: snips-nlu/\n","   creating: snips-nlu/train/\n","  inflating: snips-nlu/train/seq.in  \n","  inflating: snips-nlu/train/seq.out  \n","  inflating: snips-nlu/train/label   \n","   creating: snips-nlu/test/\n","  inflating: snips-nlu/test/seq.in   \n","  inflating: snips-nlu/test/seq.out  \n","  inflating: snips-nlu/test/label    \n","   creating: snips-nlu/valid/\n","  inflating: snips-nlu/valid/seq.in  \n","  inflating: snips-nlu/valid/seq.out  \n","  inflating: snips-nlu/valid/label   \n","   1818 AddToPlaylist\n","   1881 BookRestaurant\n","   1896 GetWeather\n","   1914 PlayMusic\n","   1876 RateBook\n","   1847 SearchCreativeWork\n","   1852 SearchScreeningEvent\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e7mon0m5kFYG"},"source":["Commençons par charger les énnoncés et étiquettes correspondant. "]},{"cell_type":"code","metadata":{"id":"781hzW9ls57B","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1579789522052,"user_tz":-60,"elapsed":1232,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"f8c8d083-74bc-469d-a746-3f0eaf4a3f7d"},"source":["def load_instances(text_filename, label_filename):\n","  with open(label_filename) as fp:\n","    labels = [line.strip() for line in fp]\n","  with open(text_filename) as fp:\n","    texts = [line.strip() for line in fp]\n","  return texts, labels\n","\n","train_texts, train_labels = load_instances('snips-nlu/train/seq.in', 'snips-nlu/train/label')\n","valid_texts, valid_labels = load_instances('snips-nlu/valid/seq.in', 'snips-nlu/valid/label')\n","test_texts, test_labels = load_instances('snips-nlu/test/seq.in', 'snips-nlu/test/label')\n","\n","for label, text in zip(train_labels[:10], train_texts[:10]):\n","  print(text, '=>', label)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["listen to westbam alumb allergic on google music => PlayMusic\n","add step to me to the 50 clásicos playlist => AddToPlaylist\n","i give this current textbook a rating value of 1 and a best rating of 6 => RateBook\n","play the song little robin redbreast => PlayMusic\n","please add iris dement to my playlist this is selena => AddToPlaylist\n","add slimm cutta calhoun to my this is prince playlist => AddToPlaylist\n","i want to listen to seventies music => PlayMusic\n","play a popular chant by brian epstein => PlayMusic\n","find fish story => SearchScreeningEvent\n","book a spot for 3 in mt => BookRestaurant\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TuKz9pRoksvD"},"source":["Il est intéressant d'afficher un histogramme des longueurs des textes pour voir quelle pourrait être une longueur maximale acceptable pour représenter les données sous forme de tenseurs. Par exemple, une longueur de 20 semble convenir pour la plus grande partie des données."]},{"cell_type":"code","metadata":{"id":"jpfnOzf_kq76","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1579789637322,"user_tz":-60,"elapsed":1148,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"0c5ca56f-f351-4ffd-e0df-828902cd765d"},"source":["from matplotlib import pyplot as plt\n","plt.hist([len(text.split()) for text in train_texts])\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"JoTCZnB2lE7F"},"source":["Il est temps d'import pytorch et de définir les classiques paramètres globaux des systèmes. Il faut noter que nous utilisons ici une grande taille de batch et l'accélérateur GPU (à activer sur colab) pour réduire les temps de traitements avec les modèles de type BERT."]},{"cell_type":"code","metadata":{"id":"DV6T-xqDvSDy"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","maxlen = 20\n","batch_size = 256\n","hidden_size = 128\n","embed_size = 128\n","device = torch.device('cuda')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YhU7XgbiltVx"},"source":["BERT est un modèle issu de la partie encodeur d'un transformer, qui repose essentiellement sur l'accumulation de couches d'attention avec un encodage de la position. On peut trouver relativement facilement sur le web des [explications illustrées](http://jalammar.github.io/illustrated-transformer/) sur les détails du fonctionnement de BERT. Nous n'allons pas le réimplémenter, mais utiliser une implémentation créée par la stratup Hugging Face. Le [répository](https://github.com/huggingface/transformers) de Hugging Face contient une implémentation de diverses variantes de modèles fondés sur les transformers ainsi que des poids préentraînés correspondant.\n","\n","Le modèle BERT fait une tokenization en sous-mots pour limiter le nombre de mots à traiter, et pour généraliser plus facilement sur les mots inconnus. Nous utiliserons cette tokenization pour toutes les exprériences. Le modèle BERT est très coûteux en temps de calcul et nous utiliserons une version plus petite, appellée [DistilBert](https://arxiv.org/abs/1910.01108), issue d'une \"distillation\" des poids du modèle BERT d'origine.\n","\n","Il est a noter que le tokenizer divise les mots qu'il ne connait pas en sous-tokens dénotés par `##`, et qu'il faut ajouter un token en début de phrase et en fin de phrase pour la limiter (le modèle est capable de traiter plusieurs phrases)."]},{"cell_type":"code","metadata":{"id":"ci8XY2bsupdw","colab":{"base_uri":"https://localhost:8080/","height":185},"executionInfo":{"status":"ok","timestamp":1579790721439,"user_tz":-60,"elapsed":10667,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"0962ec66-e531-440b-92e2-b71c19d0dd67"},"source":["!pip -q install transformers\n","from transformers import DistilBertTokenizer\n","\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","def bert_text_to_ids(sentence):\n","  return torch.tensor(tokenizer.encode(sentence, add_special_tokens=True))\n","\n","print(tokenizer.tokenize('i really like the band raspigaous.'))\n","print(bert_text_to_ids('i really like the band raspigaous.'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 450kB 8.1MB/s \n","\u001b[K     |████████████████████████████████| 1.0MB 18.9MB/s \n","\u001b[K     |████████████████████████████████| 870kB 46.0MB/s \n","\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["['i', 'really', 'like', 'the', 'band', 'ras', '##pi', '##ga', '##ous', '.']\n","tensor([  101,  1045,  2428,  2066,  1996,  2316, 20710,  8197,  3654,  3560,\n","         1012,   102])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X8zrwT7wsbQb"},"source":["Il faut maintenant créer des tenseurs avec les données de chaque sous-ensemble. BERT s'occupe de tokenizer et convertir les mots en identifiants, mais nous devons le faire pour les étiquettes. Nous en profitons pour imposer la longueur maximale et envoyer les tenseurs sur GPU."]},{"cell_type":"code","metadata":{"id":"H-FCotOavICE","colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"status":"ok","timestamp":1579790740043,"user_tz":-60,"elapsed":14835,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"ebbf624d-9db1-4f9c-97b1-0ea01a8b6573"},"source":["import collections\n","label_vocab = collections.defaultdict(lambda: len(label_vocab))\n","\n","def prepare_texts(texts, labels):\n","  X = torch.LongTensor(len(texts), maxlen).fill_(tokenizer.pad_token_id)\n","  for i, text in enumerate(texts):\n","    indexed_tokens = bert_text_to_ids(text)\n","    length = min([maxlen, len(indexed_tokens)])\n","    X[i,:length] = indexed_tokens[:length]\n","  \n","  Y = torch.tensor([label_vocab[label] for label in labels]).long()\n","  return X.to(device), Y.to(device)\n","\n","X_train, Y_train = prepare_texts(train_texts, train_labels)\n","X_valid, Y_valid = prepare_texts(valid_texts, valid_labels)\n","X_test, Y_test = prepare_texts(test_texts, test_labels)\n","\n","print(X_train.shape)\n","print(X_train[:3])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([13084, 20])\n","tensor([[  101,  4952,  2000,  2225,  3676,  2213,  2632, 25438, 27395,  2006,\n","          8224,  2189,   102,     0,     0,     0,     0,     0,     0,     0],\n","        [  101,  5587,  3357,  2000,  2033,  2000,  1996,  2753, 18856, 21369,\n","         13186,  2377,  9863,   102,     0,     0,     0,     0,     0,     0],\n","        [  101,  1045,  2507,  2023,  2783, 16432,  1037,  5790,  3643,  1997,\n","          1015,  1998,  1037,  2190,  5790,  1997,  1020,   102,     0,     0]],\n","       device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h2NgmCyusxOH"},"source":["Il s'agit maintenant de créer des datasets et générateurs de batches en utilisant l'approche classique en pytorch. "]},{"cell_type":"code","metadata":{"id":"IM4JcyFxvyMS"},"source":["from torch.utils.data import TensorDataset, DataLoader\n","train_set = TensorDataset(X_train, Y_train)\n","valid_set = TensorDataset(X_valid, Y_valid)\n","test_set = TensorDataset(X_test, Y_test)\n","\n","train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","valid_loader = DataLoader(valid_set, batch_size=batch_size)\n","test_loader = DataLoader(test_set, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qqRdrAz-tcyi"},"source":["Le premier modèle est un RNN bidirectionnel à base de GRU très classique. Il commence par une couche d'embeddings initialisés aléatoirement, suivie d'une couche de GRU, suivi de dropout et d'une couche de décision. Nous utilisons le dernier état caché du RNN comme représentation donnée à la couche de décision, dont la forme de tenseur est (nombre de couches GRU, nombre de directions, taille de l'état caché). On en déduit une taille de couche cachée de $1\\times 2 \\times \\text{hidden_size}$.\n","\n","La fonction `forward` est très classique et nécessite juste de manipuler le dernier état caché du RNN pour qu'il ait comme forme (taille du batch, taille de l'état caché $\\times$ directions $\\times$ nombre de couches). Cette fonction prend en entrée un tenseur $x$ représentant un batch de taille (taille du batch, longueur maximale d'une phrase).\n","\n","Il est toujours bien de vérifier que le tenseur renvoyé par l'inférence a pour forme (taille du batch, nombre de classes). "]},{"cell_type":"code","metadata":{"id":"yfH5mzxGE4id","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1579790747491,"user_tz":-60,"elapsed":1038,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"6cbd0802-b14b-46ce-e9f6-ecf43052ed2d"},"source":["class RNNClassifier(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.embed = nn.Embedding(tokenizer.vocab_size, embed_size, padding_idx=tokenizer.pad_token_id)\n","    self.rnn = nn.GRU(embed_size, hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n","    self.dropout = nn.Dropout(0.3)\n","    self.decision = nn.Linear(1 * 2 * hidden_size, len(label_vocab))\n","    self.to(device)\n","  \n","  def forward(self, x):\n","    embed = self.embed(x)\n","    output, hidden = self.rnn(embed) \n","    drop = self.dropout(hidden.transpose(0, 1).reshape(x.shape[0], -1))\n","    return self.decision(drop)\n","\n","rnn_model = RNNClassifier()\n","with torch.no_grad():\n","  print(rnn_model(X_train[:3]).shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([3, 7])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"k8AuJAJEvSmL"},"source":["La fonction de calcul des performances est classique. Elle utilise un critère d'entropie croisée (même loss qu'en entrînement), et calcule le taux de corrects du modèle."]},{"cell_type":"code","metadata":{"id":"4WW2YKaMJJIC","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1579790750703,"user_tz":-60,"elapsed":1091,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"7f03e875-78ff-4075-9198-4698f7a0c6ae"},"source":["def perf(model, loader):\n","    criterion = nn.CrossEntropyLoss()\n","    model.eval()\n","    total_loss = num = correct = 0\n","    for x, y in loader:\n","      with torch.no_grad():\n","        y_scores = model(x)\n","        loss = criterion(y_scores, y)\n","        y_pred = torch.max(y_scores, 1)[1]\n","        correct += torch.sum(y_pred == y).item()\n","        total_loss += loss.item()\n","        num += len(y)\n","    return total_loss / num, correct / num\n","\n","perf(rnn_model, valid_loader)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.008468323264803205, 0.19857142857142857)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"H-RQZ-nqv8qG"},"source":["La fonction d'apprentissage est elle aussi standard. Nous rendons toutefois le taux d'apprentissage (lr) paramètrable car le fine-tuning des paramètres de BERT avec Adam est instable si le taux d'apprentissage n'est pas plus bas que la valeur par défaut. Cette dernière marche très bien pour le RNN.  "]},{"cell_type":"code","metadata":{"id":"c0hg3BS4JhA7","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1579790757363,"user_tz":-60,"elapsed":4995,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"439aef08-5d65-4e30-a0e3-bc4cc0c61ff4"},"source":["def fit(model, epochs, lr=1e-3):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = num = 0\n","        for x, y in train_loader:\n","            optimizer.zero_grad()\n","            y_scores = model(x)\n","            loss = criterion(y_scores, y)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","            num += len(y)\n","        print(epoch, total_loss / num, *perf(model, valid_loader))\n","\n","fit(rnn_model, 10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 0.0036398950714796774 0.0012371916323900222 0.9228571428571428\n","1 0.0006147358687244326 0.0004416057573897498 0.9685714285714285\n","2 0.000294292799488407 0.00038191310529197967 0.9742857142857143\n","3 0.00017935920034648262 0.00048675505710499625 0.9714285714285714\n","4 0.00011342661283729192 0.00033707269600459505 0.9785714285714285\n","5 7.167282334615134e-05 0.0003726449715239661 0.98\n","6 4.8101932827972085e-05 0.0003669663997633117 0.9785714285714285\n","7 3.383737552862071e-05 0.0005068811880690711 0.97\n","8 1.7471139894947022e-05 0.0004808847246957677 0.9757142857142858\n","9 1.0764472796864983e-05 0.0005608252036784377 0.9728571428571429\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9YwHcNySVmjd"},"source":["Le second modèle est basé sur DistilBERT. L'idée est de faire passer les entrées (tokens) à travers l'encodeur d'un transformer (BERT), d'en récupérer la représentation aggrégée sur toute la phrase (équivalent de l'état caché du RNN après le dernier mot) et de la passer à une couche de décision. Il faut faire attention à dimensionner cette dernière avec la taille de la représentation générée par BERT.\n","\n","Dans la fonction forward, BERT nécessite un masque pour désigner les mots sur lesquels porte le mécanisme d'attention, sans quoi les représentations générées n'auront pas de sens. Le composant renvoie une représentation par mot et nous utilisons le max pooling pour aggréger ces représentations."]},{"cell_type":"code","metadata":{"id":"uCQwB6bKJw6S","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1579793820918,"user_tz":-60,"elapsed":2918,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"95598e7a-b403-467d-cfcf-6c5b7acb29c4"},"source":["from transformers import DistilBertModel\n","\n","class BertClassifier(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n","    self.decision = nn.Linear(self.bert.config.hidden_size, len(label_vocab))\n","    self.to(device)\n","\n","  def forward(self, x):\n","    output = self.bert(x, attention_mask = (x != tokenizer.pad_token_id).long())\n","    return self.decision(torch.max(output[0], 1)[0])\n","\n","bert_model = BertClassifier()\n","with torch.no_grad():\n","  print(bert_model(X_train[:3]).shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([3, 7])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qG3J2Vjr2WXZ"},"source":["Étant donné la longeur de l'entraînement avec BERT, nous ne ferons que 5 époque avec un taux d'apprentissage de 0.00002."]},{"cell_type":"code","metadata":{"id":"lIP7SAotPY4Y","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1579790893133,"user_tz":-60,"elapsed":117796,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"39b3981a-c9b9-4459-c134-003b4708c646"},"source":["fit(bert_model, 5, lr=2e-5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 0.0033761404866984187 0.00047010838187166624 0.9742857142857143\n","1 0.0003032651369709627 0.00026615991656269344 0.9828571428571429\n","2 0.00015907464289849265 0.0001351036104772772 0.9942857142857143\n","3 0.00010973795065704707 0.00017267641478351184 0.9885714285714285\n","4 7.808252857147836e-05 0.00020353555413229124 0.9857142857142858\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Q3sss3Sbe7m-"},"source":["Les résultats obtenus peuvent être comparés à ceux de https://github.com/czhang99/Capsule-NLU. Normalement ils devraient pas être très loin de l'état de l'art sur ce corpus."]},{"cell_type":"code","metadata":{"id":"yxZM2ermWURt","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1579790918746,"user_tz":-60,"elapsed":1549,"user":{"displayName":"Florent Charrier","photoUrl":"","userId":"14263747539054022084"}},"outputId":"7eac6668-3da4-4966-e1cd-df7755d1d3d5"},"source":["print('RNN', *perf(rnn_model, test_loader))\n","print('BERT', *perf(bert_model, test_loader))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["RNN 0.000596585784639631 0.9657142857142857\n","BERT 0.00025549729487725666 0.9757142857142858\n"],"name":"stdout"}]}]}